{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLSs1Eby_pPk"
   },
   "source": [
    "# The SVM vs. Logistic Regression Showdown\n",
    "\n",
    "In this lab, you will practice working with non-linear kernels combined with logistic regression and SVM classifiers. The goal is to compare these commonly used techniques. Which comes out on top in terms of accuracy? Runtime? Is there much of a difference at all? Is the dominance of the SVM classifier in machine learning pedagogy justified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msj5mwJs_pPl"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "First, we load all the packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5TKE1o5c_pPm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "import scipy\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L58cF4Cy_pPn"
   },
   "source": [
    "Again we download the data from the Tensorflow package, which you will need to install.  You can get the data from other sources as well.\n",
    "\n",
    "In the Tensorflow dataset, the training and test data are represented as arrays:\n",
    "\n",
    "     Xtr.shape = 60000 x 28 x 28\n",
    "     Xts.shape = 10000 x 28 x 28\n",
    "     \n",
    "The test data consists of `60000` images of size `28 x 28` pixels; the test data consists of `10000` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pigfivuh_pPn",
    "outputId": "380837e4-71bf-4368-8af4-89d821e2931f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/e4/14/d795bb156f8cc10eb1dcfe1332b7dbb8405b634688980aa9be8f885cc888/tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.16.1 from https://files.pythonhosted.org/packages/e0/36/6278e4e7e69a90c00e0f82944d8f2713dd85a69d1add455d9e50446837ab/tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.10.0 from https://files.pythonhosted.org/packages/d8/5e/b7b83cfe60504cc4d24746aed04353af7ea8ec104e597e5ae71b8d0390cb/h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/0b/2d/3f480b1e1d31eb3d6de5e3ef641954e5c67430d5ac93b7fa7e07589576c7/libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes~=0.3.1 from https://files.pythonhosted.org/packages/a4/db/1784b87285588788170f87e987bfb4bda218d62a70a81ebb66c94e7f9b95/ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/78/a9/eaa378e6fe421c2f61bdd4b92439b2b8bb320526f2b0e08fcf4e21c2f855/grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.17,>=2.16 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for keras>=3.0.0 from https://files.pythonhosted.org/packages/48/a6/ac5cc97a2677e07d3ac359d05cfb2974ec16c8c8ae6dd265e94706256608/keras-3.2.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/cd/43/b971880e2eb45c0bee2093710ae8044764a89afe9620df34a231c6f0ecd2/namex-0.0.7-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/8f/db/e05a35451d4ba30fdc65ef168dfdc68a6939ea6afdc0101e3e77f97e1547/optree-0.11.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading optree-0.11.0-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.2/46.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sky\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "   ---------------------------------------- 0.0/377.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/377.0 MB 9.6 MB/s eta 0:00:40\n",
      "   ---------------------------------------- 0.3/377.0 MB 5.2 MB/s eta 0:01:13\n",
      "   ---------------------------------------- 1.3/377.0 MB 13.8 MB/s eta 0:00:28\n",
      "   ---------------------------------------- 2.7/377.0 MB 21.4 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 4.3/377.0 MB 27.1 MB/s eta 0:00:14\n",
      "    --------------------------------------- 6.0/377.0 MB 29.3 MB/s eta 0:00:13\n",
      "    --------------------------------------- 7.7/377.0 MB 32.6 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 9.6/377.0 MB 36.1 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 11.7/377.0 MB 50.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 13.5/377.0 MB 50.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 15.4/377.0 MB 54.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 17.3/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 19.1/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 21.2/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 23.3/377.0 MB 54.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 25.3/377.0 MB 54.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 27.2/377.0 MB 59.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 29.2/377.0 MB 54.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 31.0/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 33.0/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 35.1/377.0 MB 59.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 36.7/377.0 MB 54.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 38.4/377.0 MB 50.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 40.6/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 42.8/377.0 MB 59.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 45.2/377.0 MB 54.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 47.0/377.0 MB 59.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 47.0/377.0 MB 59.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 48.9/377.0 MB 50.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 50.6/377.0 MB 46.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 52.6/377.0 MB 46.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 54.8/377.0 MB 46.7 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 56.7/377.0 MB 46.7 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 58.8/377.0 MB 59.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 60.4/377.0 MB 54.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 62.5/377.0 MB 54.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 64.0/377.0 MB 54.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 65.8/377.0 MB 50.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 67.3/377.0 MB 50.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 69.1/377.0 MB 50.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 70.5/377.0 MB 50.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 72.3/377.0 MB 50.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 74.4/377.0 MB 50.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 76.1/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 77.9/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 79.5/377.0 MB 46.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 81.4/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 83.2/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 85.1/377.0 MB 50.1 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 87.1/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 89.0/377.0 MB 50.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 90.7/377.0 MB 54.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 92.8/377.0 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 94.4/377.0 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 96.5/377.0 MB 54.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 98.8/377.0 MB 54.4 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 100.7/377.0 MB 59.5 MB/s eta 0:00:05\n",
      "   ---------- ---------------------------- 102.8/377.0 MB 59.5 MB/s eta 0:00:05\n",
      "   ---------- ---------------------------- 104.8/377.0 MB 59.5 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 107.0/377.0 MB 59.5 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 109.1/377.0 MB 59.8 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 111.2/377.0 MB 59.5 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 113.4/377.0 MB 59.5 MB/s eta 0:00:05\n",
      "   ----------- --------------------------- 116.0/377.0 MB 65.6 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 118.1/377.0 MB 65.6 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 118.6/377.0 MB 65.2 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 118.8/377.0 MB 46.7 MB/s eta 0:00:06\n",
      "   ------------ -------------------------- 119.2/377.0 MB 40.9 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 120.2/377.0 MB 36.4 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 121.2/377.0 MB 34.4 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 123.2/377.0 MB 34.4 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 124.8/377.0 MB 34.4 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 126.5/377.0 MB 32.7 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 128.2/377.0 MB 31.2 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 130.4/377.0 MB 50.4 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 132.1/377.0 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 132.1/377.0 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 132.1/377.0 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 133.2/377.0 MB 38.6 MB/s eta 0:00:07\n",
      "   ------------- ------------------------- 134.1/377.0 MB 34.4 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 135.0/377.0 MB 31.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 136.6/377.0 MB 32.8 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 138.8/377.0 MB 32.7 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 141.0/377.0 MB 32.7 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 142.5/377.0 MB 40.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 144.6/377.0 MB 50.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 146.8/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   --------------- ----------------------- 147.8/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   --------------- ----------------------- 149.6/377.0 MB 54.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 151.6/377.0 MB 50.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 153.5/377.0 MB 54.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 155.5/377.0 MB 54.7 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 157.6/377.0 MB 50.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 159.6/377.0 MB 54.7 MB/s eta 0:00:04\n",
      "   ---------------- ---------------------- 161.8/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   ---------------- ---------------------- 163.5/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 166.4/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 168.6/377.0 MB 65.6 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 170.9/377.0 MB 65.6 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 173.0/377.0 MB 65.6 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 173.0/377.0 MB 65.6 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 175.0/377.0 MB 50.1 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 177.0/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 179.3/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 181.9/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 184.0/377.0 MB 65.6 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 185.9/377.0 MB 65.6 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 188.0/377.0 MB 65.2 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 190.1/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 192.2/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 194.5/377.0 MB 59.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 196.6/377.0 MB 65.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 198.4/377.0 MB 65.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 200.6/377.0 MB 59.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 202.7/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 204.2/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 206.1/377.0 MB 54.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 208.2/377.0 MB 54.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 210.5/377.0 MB 54.7 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 212.7/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 214.8/377.0 MB 65.6 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 215.7/377.0 MB 54.4 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 217.2/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 219.0/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 221.3/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 223.2/377.0 MB 50.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 225.6/377.0 MB 54.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 227.6/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 229.8/377.0 MB 65.6 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 232.0/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 234.2/377.0 MB 65.6 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 236.1/377.0 MB 59.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 238.2/377.0 MB 65.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 240.3/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 242.1/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 243.9/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 246.3/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 248.4/377.0 MB 54.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 250.5/377.0 MB 59.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 252.5/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 254.4/377.0 MB 59.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 256.4/377.0 MB 54.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 258.7/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 260.9/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 263.2/377.0 MB 65.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 265.6/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 267.6/377.0 MB 65.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 270.0/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 272.3/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 274.4/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 276.4/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 278.6/377.0 MB 65.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 280.6/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 282.5/377.0 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 284.7/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 287.0/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 289.3/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 291.5/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 293.6/377.0 MB 65.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 295.7/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 297.9/377.0 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 300.3/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 302.0/377.0 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 303.7/377.0 MB 59.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 304.6/377.0 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 306.7/377.0 MB 50.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 308.7/377.0 MB 50.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 310.4/377.0 MB 46.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 312.8/377.0 MB 50.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 314.5/377.0 MB 54.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 316.5/377.0 MB 54.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 318.5/377.0 MB 54.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 319.9/377.0 MB 50.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 321.4/377.0 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 323.3/377.0 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 325.3/377.0 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 327.6/377.0 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 329.6/377.0 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 331.8/377.0 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 334.2/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 336.7/377.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 339.0/377.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 341.1/377.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 343.1/377.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 345.3/377.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 347.4/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 349.8/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 351.9/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 354.2/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 356.4/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 358.7/377.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.1/377.0 MB 59.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 362.0/377.0 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 363.8/377.0 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.8/377.0 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  367.8/377.0 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  370.2/377.0 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.1/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.2/377.0 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.5/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 377.0/377.0 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 2.1/3.8 MB 67.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 60.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 48.4 MB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.0/3.0 MB 33.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.9/3.0 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 31.6 MB/s eta 0:00:00\n",
      "Downloading keras-3.2.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 33.4 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.0/26.4 MB 63.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/26.4 MB 45.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 6.0/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.0/26.4 MB 57.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.6/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.2/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.4/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 16.5/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.8/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.4/26.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.5/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 46.7 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.7/127.7 kB ? eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 413.4/413.4 kB ? eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 67.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 74.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 58.3 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 47.6 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp311-cp311-win_amd64.whl (245 kB)\n",
      "   ---------------------------------------- 0.0/245.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 245.0/245.0 kB ? eta 0:00:00\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 240.7/240.7 kB ? eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.11.0 keras-3.2.1 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Xtr shape: (60000, 28, 28)\n",
      "Xts shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "(Xtr_raw,ytr),(Xts_raw,yts) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Xtr shape: %s' % str(Xtr_raw.shape))\n",
    "print('Xts shape: %s' % str(Xts_raw.shape))\n",
    "\n",
    "ntr = Xtr_raw.shape[0]\n",
    "nts = Xts_raw.shape[0]\n",
    "nrow = Xtr_raw.shape[1]\n",
    "ncol = Xtr_raw.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgE5TO4b_pPo"
   },
   "source": [
    "Each pixel value is from `[0,255]`.  For this lab, it will be convenient to recale the value to -1 to 1 and reshape it as a `ntr x npix` and `nts x npix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahZI8kazNihC",
    "outputId": "d01e4339-ba44-4294-f048-e71a835c590f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.3         0.12352941  0.49215686  0.12352941 -0.30392157\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31176471  0.43333333\n",
      "  0.48823529  0.48823529  0.48823529  0.42941176 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.28823529  0.39019608  0.49215686  0.48823529  0.4372549\n",
      "  0.41372549  0.48823529 -0.27647059 -0.47647059 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.46078431 -0.26470588  0.37843137\n",
      "  0.48823529  0.49215686  0.48823529  0.29215686 -0.17058824  0.48823529\n",
      "  0.49215686 -0.02156863 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5         0.13921569  0.48823529  0.48823529  0.48823529  0.49215686\n",
      "  0.48823529  0.48823529 -0.12352941  0.24117647  0.49215686  0.15490196\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.3         0.43333333\n",
      "  0.49215686  0.49215686  0.24509804 -0.05294118  0.49215686  0.39411765\n",
      " -0.31568627 -0.19019608  0.5         0.15882353 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.31176471  0.43333333  0.48823529  0.48823529  0.20196078\n",
      " -0.45294118 -0.20588235 -0.0254902  -0.41764706 -0.5        -0.5\n",
      "  0.49215686  0.45294118 -0.30392157 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.35098039  0.14705882\n",
      "  0.49215686  0.41372549  0.31568627 -0.17058824 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5         0.49215686  0.48823529\n",
      "  0.14705882 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.47254902  0.19803922  0.48823529  0.44117647 -0.22156863\n",
      " -0.4254902  -0.39019608 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.49215686  0.48823529  0.26470588 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.27647059\n",
      "  0.48823529  0.48823529 -0.25294118 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  0.49215686  0.48823529  0.26470588 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5         0.27647059  0.49215686  0.24509804\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5         0.5         0.49215686\n",
      "  0.26862745 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.20196078  0.46470588  0.48823529 -0.06078431 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.49215686  0.48823529  0.08039216 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.16666667  0.48823529\n",
      "  0.40196078 -0.40196078 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.47254902  0.02941176\n",
      "  0.49215686  0.22941176 -0.45294118 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.16666667  0.48823529  0.3745098  -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.47254902  0.01372549  0.48823529  0.38235294 -0.22156863\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.16666667  0.48823529  0.06862745 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31176471  0.14705882\n",
      "  0.48823529  0.17843137 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.1627451   0.49215686\n",
      "  0.38235294 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.05294118  0.43333333  0.49215686  0.13529412 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.16666667  0.48823529  0.47647059  0.07254902\n",
      " -0.31176471 -0.38627451 -0.16666667  0.19803922  0.38235294  0.49215686\n",
      "  0.3745098   0.15490196 -0.28039216 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.16666667  0.48823529  0.48823529  0.48823529  0.39803922  0.34313725\n",
      "  0.48823529  0.48823529  0.48823529  0.26862745  0.00980392 -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.39019608  0.28039216\n",
      "  0.48823529  0.48823529  0.49215686  0.48823529  0.48823529  0.41372549\n",
      "  0.06862745 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.40196078  0.00196078  0.48823529\n",
      "  0.49215686  0.48823529  0.05294118 -0.35490196 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "npix = nrow*ncol\n",
    "Xtr = Xtr_raw.reshape((ntr,npix))\n",
    "print(Xtr[1,:])\n",
    "Xtr = (Xtr/255 - .5)\n",
    "print(Xtr[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QE-50PdU_pPo"
   },
   "outputs": [],
   "source": [
    "npix = nrow*ncol\n",
    "Xtr = (Xtr_raw/255 - 0.5)\n",
    "Xtr = Xtr.reshape((ntr,npix))\n",
    "\n",
    "Xts = (Xts_raw/255 - 0.5)\n",
    "Xts = Xts.reshape((nts,npix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqDWXC9yN6JH",
    "outputId": "5d65352c-ee7c-49b7-e541-2a0aa994eb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " ...\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frhehbKs_pPo"
   },
   "source": [
    "For this lab we're only going to use a fraction of the MNIST data -- otherwise our models will take too much time and memory to run. Using only part of the training data will of course lead to worse results. Given enough computational resources and time, we would ideally be running on the full data set. The follow code creates a new test and train set, with 10000 examples for train and 5000 for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "cUHbBwL1_pPp"
   },
   "outputs": [],
   "source": [
    "ntr1 = 10000\n",
    "nts1 = 5000\n",
    "Iperm = np.random.permutation(ntr1)\n",
    "Xtr1 = Xtr[Iperm[:ntr1],:]\n",
    "ytr1 = ytr[Iperm[:ntr1]]\n",
    "Iperm = np.random.permutation(nts1)\n",
    "Xts1 = Xts[Iperm[:nts1],:]\n",
    "yts1 = yts[Iperm[:nts1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " ...\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ... -0.5 -0.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(Xts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlz6g-X8_pPq"
   },
   "source": [
    "## Problem set up and establishing a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T90pjHcr_pPq"
   },
   "source": [
    "To simplify the problem (and speed things up) we're also going to restrict to binary classification. In particular, let's try to design classifier a that separates the 8's from all other digits.\n",
    "\n",
    "Create binary 0/1 label vectors `ytr8` and `yts8` which are 1 wherever `ytr1` and `yts1` equal 8, and 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "q6Flo-KS_pPq"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "ytr8 = (ytr1 == 8).astype(int)\n",
    "yts8 = (yts1 == 8).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdT2KGyj_pPr"
   },
   "source": [
    "Most of the digits in the test dataset aren't equal to 8. So if we simply guess 0 for every image in `Xts`, we might expect to get classification accuracy around 90%. Our goal should be to significantly beat this **baseline**.\n",
    "\n",
    "Formally, write a few lines of code to check what test error would be achieved by the all zeros classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "3JweYada_pPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.902200\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "predict = np.zeros(len(Xts))\n",
    "acc = np.mean(yts8 == 0)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz_7u10V_pPr"
   },
   "source": [
    "As a second baseline, let's see how we do with standard (non-kernel) logistic regression. As in the MNIST demo, you can use `scikit-learn`'s built in function `linear_model.LogisticRegression` to fit the model and compute the accuracy. Use no regularization and the `lbfgs` solver. You should acheive an improvement to around 93-95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "oocR_r9C_pPr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sky\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100000.0, max_iter=100000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100000.0, max_iter=100000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100000.0, max_iter=100000)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(C=1e5, solver='lbfgs', max_iter=100000)\n",
    "model.fit(Xtr1, ytr8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuaracy = 0.929600\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(Xts1)\n",
    "acc = accuracy_score(yts8, predictions)\n",
    "print('Logistic Regression Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXuJVilo_pPr"
   },
   "source": [
    "## Kernel Logistic Regression\n",
    "\n",
    "To improve on this baseline performance, let's try using the logistic regression classifier with a *non-linear* kernel. Recall from class that any non-linear kernel similarity function $k(\\vec{w},\\vec{z})$ is equal to $\\phi(\\vec{w})^T\\phi(\\vec{z})$ for some feature transformation $\\phi$. However, we typically do not need to compute this feature tranformation explicitly: instead we can work directly with the kernel gram matrix $K \\in \\mathbb{R}^{n\\times n}$. Recall that $K_{i,j} = k(\\vec{x}_i,\\vec{x}_j)$ where $\\vec{x}_i$ is the $i^\\text{th}$ training data point.\n",
    "\n",
    "For this lab we will be using the radial basis function kernel. For a given scaling factor $\\gamma$ this kernel is defined as:\n",
    "$$\n",
    "k(\\vec{w},\\vec{z}) = e^{-\\gamma\\|\\vec{w}-\\vec{z}\\|_2^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "q8znIoWi_pPs"
   },
   "outputs": [],
   "source": [
    "def rbf_kernel(w,z,gamma):\n",
    "    d = w - z\n",
    "    return np.exp(-gamma*np.sum(d*d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WFi15JF_pPs"
   },
   "source": [
    "Construct the kernel matrix `K1` for `Xtr1` with `gamma = .05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def compute_kernel_matrix(X, gamma):\n",
    "    # Compute the squared Euclidean distance matrix\n",
    "    sq_dists = cdist(X, X, 'sqeuclidean')\n",
    "    # Compute the kernel matrix\n",
    "    K = np.exp(-gamma * sq_dists)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "9PBzA-5__pPs"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "gamma = .05\n",
    "K1 = compute_kernel_matrix(Xtr1, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.02510458 0.00338946 ... 0.01088088 0.00380884 0.009766  ]\n",
      " [0.02510458 1.         0.019551   ... 0.00734931 0.00976997 0.01208535]\n",
      " [0.00338946 0.019551   1.         ... 0.00378603 0.01146406 0.00212038]\n",
      " ...\n",
      " [0.01088088 0.00734931 0.00378603 ... 1.         0.00744453 0.01824785]\n",
      " [0.00380884 0.00976997 0.01146406 ... 0.00744453 1.         0.00549742]\n",
      " [0.009766   0.01208535 0.00212038 ... 0.01824785 0.00549742 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(K1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB_X-JOR_pPs"
   },
   "source": [
    "If you used a for loop (which is fine) your code might take several minutes to run! Part of the issue is that Python won't know to properly parallize your for loop. For this reason, when constructing kernel matrices it is often faster to us a built-in, carefully optimized function with explicit parallelization. Scikit learn provides such a function through their `metrics` library.\n",
    "\n",
    "Referring to the documentation here\n",
    "https://scikit-learn.org/stable/modules/metrics.html#metrics, use this built in function to recreate the same kernel matrix you did above. Store the result at `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "Nhd-xFsn_pPt"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "K = rbf_kernel(Xtr1, Xtr1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.02510458, 0.00338946, ..., 0.01088088, 0.00380884,\n",
       "        0.009766  ],\n",
       "       [0.02510458, 1.        , 0.019551  , ..., 0.00734931, 0.00976997,\n",
       "        0.01208535],\n",
       "       [0.00338946, 0.019551  , 1.        , ..., 0.00378603, 0.01146406,\n",
       "        0.00212038],\n",
       "       ...,\n",
       "       [0.01088088, 0.00734931, 0.00378603, ..., 1.        , 0.00744453,\n",
       "        0.01824785],\n",
       "       [0.00380884, 0.00976997, 0.01146406, ..., 0.00744453, 1.        ,\n",
       "        0.00549742],\n",
       "       [0.009766  , 0.01208535, 0.00212038, ..., 0.01824785, 0.00549742,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO9CnBbN_pPt"
   },
   "source": [
    "Check that you used the function correctly by writing code to confirm that `K = K1`, or at least that the two are equal up to very small differences (which could arise due to numerical precision issues). Try to do this **without a for loop** for full credit. You will get partial credit if you use a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "SCOfrRRD_pPt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "are_close = np.allclose(K, K1, atol=1e-10)\n",
    "print(are_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrIT1xJL_pPt"
   },
   "source": [
    "When using a non-linear kernel, it is important to check that you have chosen reasonable parameters (in our case the only parameter is `gamma`). We typically do not want $k(\\vec{x}_i,\\vec{x}_j)$ to be either negligably small, or very large for all $i\\neq j$ in our data set, or we won't be able to learn anything. For the RBF kernel this means that, for any $\\vec{x}_i$ we don't want $k(\\vec{x}_i,\\vec{x}_j)$ very close to 1 (e.g. .9999) for all $j$, or very close to $0$ (e.g. 1e-8) for all $j$.\n",
    "\n",
    "Let's just check that we're in good shape for the first data vector $\\vec{x}_0$. Do so by printing out the 10 largest and 10 smallest values of $k(\\vec{x}_0,\\vec{x}_j)$ for $j\\neq 0$. Note that we always have $k(\\vec{x}_0,\\vec{x}_0) = 1$ for the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "z2yZ-vdt_pPt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarities: \n",
      " [0.16667669 0.10317341 0.08614872 0.08451159 0.08234248 0.07835038\n",
      " 0.07764239 0.07746367 0.07684656 0.07236095]\n",
      "Minimum similarities: \n",
      " [2.60692441e-05 3.19661095e-05 3.41648853e-05 3.67479575e-05\n",
      " 3.96780043e-05 3.98211699e-05 4.25023671e-05 4.27136539e-05\n",
      " 4.77027215e-05 5.05935594e-05]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "kernel_values = K[0, 1:]\n",
    "\n",
    "# Find the 10 largest values excluding the self-comparison\n",
    "largest = np.partition(kernel_values, -10)[-10:]\n",
    "smallest = np.partition(kernel_values, 10)[:10]\n",
    "\n",
    "print('Maximum similarities: \\n', np.sort(largest)[::-1])\n",
    "print('Minimum similarities: \\n', np.sort(smallest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0Vfxdfu_pPu"
   },
   "source": [
    "### Implementation\n",
    "Maybe surprisingly Scikit learn does not have an implementation for kernel logistic regression. So we have to implement our own!\n",
    "\n",
    "Write a function function `log_fit` that minimizes the $\\ell_2$-regularized logisitic regression loss:\n",
    "$$\n",
    "L(\\boldsymbol{\\alpha}) =\\sum_{i=1}^n (1-y_i)(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{X})^T\\vec{\\alpha}) - \\log(h(\\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{X})^T\\boldsymbol{\\alpha}) + \\lambda \\|\\phi(\\mathbf{X})^T\\boldsymbol{\\alpha}\\|_2^2.\n",
    "$$\n",
    "As input it takes an $n\\times n$ kernel matrix $K$ for the training data, an $n$ length vector `y` of binary class labels, and regularization parameter `lamb`.\n",
    "\n",
    "To implement your function you can either use your own implmentation of gradient descent or used a built in minimizer from `scipy.optimize.minimize`. I recommend the later approach: as we saw in the last lab, gradient descent can converge slowly for this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized alpha: [-0.40934477 -0.30785323 -1.05335576 ... -0.19968716 -0.15283513\n",
      "  0.07462778]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(alpha, K, y, lamb):\n",
    "    linear_combination = K @ alpha\n",
    "    predictions = sigmoid(linear_combination)\n",
    "    # Regular logistic loss\n",
    "    logistic_loss = np.sum((1 - y) * linear_combination - np.log(predictions + 1e-10))\n",
    "    # Regularization term\n",
    "    regularization = lamb * (alpha.T @ K @ alpha)\n",
    "    return logistic_loss + regularization\n",
    "\n",
    "def logistic_loss_grad(alpha, K, y, lamb):\n",
    "    predictions = sigmoid(K @ alpha)\n",
    "    grad_loss = K.T @ (predictions - y)\n",
    "    grad_reg = 2 * lamb * (K @ alpha)\n",
    "    return grad_loss + grad_reg\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def log_fit(K, y, lamb):\n",
    "    n = len(y)  # Number of training samples\n",
    "    alpha_init = np.zeros(n)  # Initial guess for alpha\n",
    "\n",
    "    # Minimization process\n",
    "    result = minimize(\n",
    "        fun=logistic_loss,       # The loss function to minimize\n",
    "        x0=alpha_init,           # Initial guess\n",
    "        args=(K, y, lamb),       # Additional arguments for the loss function\n",
    "        method='L-BFGS-B',       # Optimization method\n",
    "        jac=logistic_loss_grad,   # Gradient of the loss function\n",
    "        options={'maxls': 50}\n",
    "    )\n",
    "\n",
    "    if result.success:\n",
    "        return result.x  # Return the optimized alpha\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed: \" + result.message)\n",
    "        \n",
    "alpha_opt = log_fit(K, ytr8,0)\n",
    "print(\"Optimized alpha:\", alpha_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC4MZYmXMAmY"
   },
   "source": [
    "Use the `log_fit` function defined above to find parameters `alpha` for the kernel logistic regression model using `lamb = 0` and `K` as constructed above (with `gamma = .05`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZXI3kz2_pPu"
   },
   "source": [
    "Suppose we have a test dataset with $m$ examples $\\vec{w}_1,\\ldots, \\vec{w}_m$. Once we obtain a coefficient vector $\\alpha$, making predictions for any $\\vec{w}_j$ in the test set requires computing:\n",
    "$$\n",
    "{y}_{j} = \\sum_{i=1}^n \\alpha_i \\cdot k(\\vec{w}_{j}, \\vec{x}_i).\n",
    "$$\n",
    "where $\\vec{x}_1, \\ldots \\vec{x}_n$ are our training data vectors. We classify $\\vec{w}_{j}$ in class 0 if ${y}_{j} \\leq 0$ and in class 1 if ${y}_{j} > 0$.\n",
    "\n",
    "This computation can be rewritten in matrix form as follows:\n",
    "$$\n",
    "\\vec{y}_{test} = K_{test}\\vec{\\alpha},\n",
    "$$\n",
    "where $\\vec{y}_{text}$ is an $m$ length vector and $K_{test}$ is a $m\\times n$ matrix whose $(j,i)$ entry is equal to $k(\\vec{w}_{j}, \\vec{x}_i)$. We classify $\\vec{w}_{j}$ in class 0 if $\\vec{y}_{test}[j] \\leq 0$ and in class 1 if $\\vec{y}_{test}[j] > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6BhIPrH_pPv"
   },
   "source": [
    "Use the `pairwise_kernels` function to construct $K_{test}$. Then make predictions for the test set and evaluate the accuracy of our kernel logistic regression classifier. You should see a pretty substantial lift in accuracy to around $97\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "GrAoBfvK_pPv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.970400\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "K_train = rbf_kernel(Xtr1, Xtr1, gamma=0.05)\n",
    "\n",
    "# Minimize the logistic loss function to get alpha coefficients\n",
    "alpha_opt = log_fit(K_train,ytr8,0)\n",
    "        \n",
    "\n",
    "# Compute the test kernel matrix\n",
    "K_test = rbf_kernel(Xts1, Xtr1, 0.05)\n",
    "\n",
    "# Make predictions for the test set\n",
    "y_test_pred = np.dot(K_test, alpha_opt)\n",
    "yhat = ( y_test_pred >= 0).astype(int)\n",
    "acc = np.mean(yhat == yts8)\n",
    "\n",
    "print(\"Test accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2SO0yhM_pPv"
   },
   "source": [
    "## Kernel Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9x1jZnh_pPv"
   },
   "source": [
    "The goal of this lab is to compare Kernel Logistic Regression to Kernel Support Vector machines. Following `demo_mnist_svm.ipynb` create and train an SVM classifier on `Xtr1` and `ytr8` using an RBF kernel with `gamma = .05` (the same value we used for logistic regression aboe). Use margin parameter `C = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "7r-c_g2z_pPv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.05)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, gamma=0.05)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create the SVM classifier with RBF kernel\n",
    "svm_classifier = SVC(kernel='rbf', gamma=0.05, C=10)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(Xtr1, ytr8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hcOkVNj_pPw"
   },
   "source": [
    "Calculate and print the accuracy of the SVM classifier. You should obtain a similar result as for logistic regression: something close to $97\\%$ accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "o5nSyO3J_pPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.980200\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ysvm = svm_classifier.predict(Xts1)\n",
    "acc = np.mean(ysvm == yts8)\n",
    "print(\"Test accuracy = %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWp9NlMa_pPw"
   },
   "source": [
    "## The Showdown\n",
    "\n",
    "Both SVM classifiers and kernel logisitic regression require tuning parameters to obtain the best possible result. In our setting we will stick with an RBF kernel (although this could be tuned). So we only consider tuning the kernel width parameter `gamma`, as well as the regularization parameter `lamb` for logistic regression, and the margin parameter `C` for SVM. We will choose parameters using for-loops and train-test cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia44LYbc_pPw"
   },
   "source": [
    "Train a logistic regression classifier with **all combinations** of the parameters included below in vectors `gamma` and `lamb`. For each setting of parameters, compute and print:\n",
    "* the test error obtained\n",
    "* the total runtime of classification in seconds (including training time and prediction time)\n",
    "\n",
    "For computing runtime you might want to use the `time()` function from the `time` library, which we already imported ealier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "0cHNr0Kw_pPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma: 0.1, Lambda: 0, Test Error: 0.0392, Accuracy: 0.9608, Total Runtime: 3.3166 seconds\n",
      "Gamma: 0.1, Lambda: 1e-06, Test Error: 0.0392, Accuracy: 0.9608, Total Runtime: 3.0925 seconds\n",
      "Gamma: 0.1, Lambda: 0.0001, Test Error: 0.0392, Accuracy: 0.9608, Total Runtime: 3.0490 seconds\n",
      "Gamma: 0.1, Lambda: 0.01, Test Error: 0.0566, Accuracy: 0.9434, Total Runtime: 2.9837 seconds\n",
      "Gamma: 0.05, Lambda: 0, Test Error: 0.0296, Accuracy: 0.9704, Total Runtime: 3.7466 seconds\n",
      "Gamma: 0.05, Lambda: 1e-06, Test Error: 0.0296, Accuracy: 0.9704, Total Runtime: 3.7615 seconds\n",
      "Gamma: 0.05, Lambda: 0.0001, Test Error: 0.0296, Accuracy: 0.9704, Total Runtime: 3.9005 seconds\n",
      "Gamma: 0.05, Lambda: 0.01, Test Error: 0.0258, Accuracy: 0.9742, Total Runtime: 24.9651 seconds\n",
      "Gamma: 0.02, Lambda: 0, Test Error: 0.0228, Accuracy: 0.9772, Total Runtime: 8.3309 seconds\n",
      "Gamma: 0.02, Lambda: 1e-06, Test Error: 0.0232, Accuracy: 0.9768, Total Runtime: 7.9402 seconds\n",
      "Gamma: 0.02, Lambda: 0.0001, Test Error: 0.0254, Accuracy: 0.9746, Total Runtime: 8.1544 seconds\n",
      "Gamma: 0.02, Lambda: 0.01, Test Error: 0.0172, Accuracy: 0.9828, Total Runtime: 210.7146 seconds\n",
      "Gamma: 0.01, Lambda: 0, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 3.0807 seconds\n",
      "Gamma: 0.01, Lambda: 1e-06, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 3.2116 seconds\n",
      "Gamma: 0.01, Lambda: 0.0001, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 3.1503 seconds\n",
      "Gamma: 0.01, Lambda: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 3.2399 seconds\n",
      "Gamma: 0.005, Lambda: 0, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 2.7652 seconds\n",
      "Gamma: 0.005, Lambda: 1e-06, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 2.7669 seconds\n",
      "Gamma: 0.005, Lambda: 0.0001, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 2.7624 seconds\n",
      "Gamma: 0.005, Lambda: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Total Runtime: 3.0849 seconds\n"
     ]
    }
   ],
   "source": [
    "gammas = [.1, .05,.02,.01,.005]\n",
    "lambs = [0,1e-6,1e-4,1e-2]\n",
    "for gamma in gammas:\n",
    "    for lamb in lambs:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        K_train = rbf_kernel(Xtr1, Xtr1, gamma=gamma)\n",
    "\n",
    "        # Minimize the logistic loss function to get alpha coefficients\n",
    "        alpha_opt = log_fit(K_train,ytr8,lamb)\n",
    "        \n",
    "\n",
    "        # Compute the test kernel matrix\n",
    "        K_test = rbf_kernel(Xts1, Xtr1, gamma)\n",
    "\n",
    "        # Make predictions for the test set\n",
    "        y_test_pred = np.dot(K_test, alpha_opt)\n",
    "        yhat = ( y_test_pred >= 0).astype(int)\n",
    "        acc = np.mean(yhat == yts8)\n",
    "        test_error = 1 - acc\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Gamma: {gamma}, Lambda: {lamb}, Test Error: {test_error:.4f}, Accuracy: {acc:.4f}, Total Runtime: {runtime:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkcP20AM_pPw"
   },
   "source": [
    "TODO: What was the best test error achieved, and what setting of parameters achieved this error? Was the kernel logistic regression classifier more sensitive to changes in `gamma` or `lamb`? Discuss in 1-3 short sentences below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best test error achieved was 0.0172, obtained with Gamma: 0.02 and Lambda: 0.01. This setting provided the highest accuracy (0.9828) within the explored parameter grid.\n",
    "\n",
    "Regarding sensitivity, the kernel logistic regression classifier appears to be more sensitive to changes in gamma than lambda since changes in gamma lead to more significant fluctuations in performance compared to changes in lambda. It's also noteworthy that for the same gamma value, different lambda values do not radically change the test error, indicating a stronger influence of the kernel parameter on the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHxF0MXm_pPw"
   },
   "source": [
    "Now let's do the same thing for the kernel Support Vector Classifier. Train an SVM classifier with **all combinations** of the parameters included below in vectors `gamma` and `C`. For each setting of parameters, compute:\n",
    "* the test error obtained\n",
    "* the total runtime of classification in seconds (including training time and prediction time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "FkPkZGIY_pPx"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "gammas = [.1, .05,.02,.01,.005]\n",
    "Cs = [.01,.1,1,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma: 0.1, C: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Runtime: 68.49 seconds\n",
      "Gamma: 0.1, C: 0.1, Test Error: 0.0978, Accuracy: 0.9022, Runtime: 106.47 seconds\n",
      "Gamma: 0.1, C: 1, Test Error: 0.0718, Accuracy: 0.9282, Runtime: 121.73 seconds\n",
      "Gamma: 0.1, C: 10, Test Error: 0.0658, Accuracy: 0.9342, Runtime: 148.04 seconds\n",
      "Gamma: 0.05, C: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Runtime: 22.82 seconds\n",
      "Gamma: 0.05, C: 0.1, Test Error: 0.0772, Accuracy: 0.9228, Runtime: 25.26 seconds\n",
      "Gamma: 0.05, C: 1, Test Error: 0.0226, Accuracy: 0.9774, Runtime: 29.44 seconds\n",
      "Gamma: 0.05, C: 10, Test Error: 0.0198, Accuracy: 0.9802, Runtime: 30.22 seconds\n",
      "Gamma: 0.02, C: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Runtime: 16.12 seconds\n",
      "Gamma: 0.02, C: 0.1, Test Error: 0.0488, Accuracy: 0.9512, Runtime: 14.48 seconds\n",
      "Gamma: 0.02, C: 1, Test Error: 0.0180, Accuracy: 0.9820, Runtime: 11.02 seconds\n",
      "Gamma: 0.02, C: 10, Test Error: 0.0142, Accuracy: 0.9858, Runtime: 11.47 seconds\n",
      "Gamma: 0.01, C: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Runtime: 16.17 seconds\n",
      "Gamma: 0.01, C: 0.1, Test Error: 0.0550, Accuracy: 0.9450, Runtime: 14.20 seconds\n",
      "Gamma: 0.01, C: 1, Test Error: 0.0240, Accuracy: 0.9760, Runtime: 9.57 seconds\n",
      "Gamma: 0.01, C: 10, Test Error: 0.0130, Accuracy: 0.9870, Runtime: 8.81 seconds\n",
      "Gamma: 0.005, C: 0.01, Test Error: 0.0978, Accuracy: 0.9022, Runtime: 15.82 seconds\n",
      "Gamma: 0.005, C: 0.1, Test Error: 0.0680, Accuracy: 0.9320, Runtime: 15.24 seconds\n",
      "Gamma: 0.005, C: 1, Test Error: 0.0328, Accuracy: 0.9672, Runtime: 10.45 seconds\n",
      "Gamma: 0.005, C: 10, Test Error: 0.0162, Accuracy: 0.9838, Runtime: 8.28 seconds\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    for C in Cs:\n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        # Create and train the SVM classifier\n",
    "        classifier = SVC(kernel='rbf', gamma=gamma, C=C)\n",
    "        classifier.fit(Xtr1, ytr8)  # Training the classifier\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = classifier.predict(Xts1)\n",
    "\n",
    "        # Calculate the test error\n",
    "        accuracy = accuracy_score(yts8, y_pred)\n",
    "        test_error = 1 - accuracy\n",
    "        \n",
    "        end_time = time.time()  # End timing\n",
    "        runtime = end_time - start_time  # Calculate runtime\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'gamma': gamma,\n",
    "            'C': C,\n",
    "            'test_error': test_error,\n",
    "            'runtime': runtime,\n",
    "            'acc': accuracy\n",
    "        })\n",
    "\n",
    "        # Optionally print results\n",
    "        print(f\"Gamma: {gamma}, C: {C}, Test Error: {test_error:.4f}, Accuracy: {accuracy:.4f}, Runtime: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPI-k50w_pPx"
   },
   "source": [
    "TODO: What was the best test error achieved, and what setting of parameters achieved this error? Which performed better in terms of accuracy, the SVM or logisitic regression classifier? How about in terms of runtime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Param Combo for SVM is: Gamma: 0.01, C: 10, Test Error: 0.0130, Accuracy: 0.9870, Runtime: 8.81 seconds\n",
    "In terms of accuracy, the SVM classifier performed \"slightly\" better than the kernel logistic regression classifier. \n",
    "In terms of runtime, LogReg runs significantly faster on average. However, for some reason, the best performing model in LogReg took 210s to run. I am not sure whether it is because of my computer or it is supposed to be like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wec-DXhg_pPx"
   },
   "source": [
    "**NOTE:** For `sklearns`'s built in classifiers, including svm.SVC, there is a function called `GridSearchCV` which can automatically perform hyperparamater tuning for you. The main advantage of the method (as opposed to writing for-loops) is that it supports parallelization, so it can fit with different parameters at the same time. The function also supports automatic $k$-fold cross-validation (instead of simple train/test split).\n",
    "\n",
    "You might be interested in using this function in the future. If so, please check out the tutorial in the following lab from previous year: https://github.com/sdrangan/introml/blob/master/unit08_svm/lab_emnist_partial.ipynb."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
