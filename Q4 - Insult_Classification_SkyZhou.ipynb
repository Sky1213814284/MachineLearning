{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insult Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we would like to filter out insulting comments on a web forum. \n",
    "\n",
    "To train our models, we have a list of historic comments with a judgement wether they're insulting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>You fuck your dad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>i really don't understand your point.  It seem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                                 You fuck your dad.\n",
       "1       0  20120528192215Z  i really don't understand your point.  It seem..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path_to_insults = \"C://Users//Sky//Downloads//data//data//\"\n",
    "data = pd.read_csv(path_to_insults + 'train-utf8.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3947 comments, of which 1049 insults (26%)\n"
     ]
    }
   ],
   "source": [
    "print (\"%d comments, of which %d insults (%d%%)\" % \\\n",
    "    (len(data), data.Insult.sum(), 100 * data.Insult.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for known bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do this, is to load Google's bad word list and flag comments that contain one or more words.\n",
    "\n",
    "- Load `google_badlist.txt` from `data/insults/`\n",
    "- Add a column to `data` with a flag (0 or 1) if the comment contains a bad word\n",
    "- Compute the accuracy of this method - does this look good?\n",
    "- What would a naive classifier's score be (i.e., always predicting 0 or 1)?\n",
    "- Also compute the precision, recall, F1 score and AUC score\n",
    "- What is your verdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C://Users//Sky//Downloads//data//data//google_badlist.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = path_to_insults + 'google_badlist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6559412211806436\n",
      "F1 Score: 0.4080209241499564\n",
      "AUC Score: 0.5890116190713032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# Load the bad words list\n",
    "with open(filename, 'r') as file:\n",
    "    bad_words = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Function to flag comments with bad words\n",
    "def flag_comment(comment):\n",
    "    return any(bad_word in comment for bad_word in bad_words)\n",
    "\n",
    "# Flag each comment\n",
    "data['flag'] = data['Comment'].apply(flag_comment).astype(int)\n",
    "\n",
    "# Compute accuracy if you have true labels\n",
    "true_labels = data['Insult']\n",
    "accuracy = accuracy_score(true_labels, data['flag'])\n",
    "f1 = f1_score(true_labels, data['flag'])\n",
    "auc = roc_auc_score(true_labels, data['flag'])\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'AUC Score: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### This method provides a naive implementation of insult recognition and produces a better accuracy than a random guess. This method is intuitive but not satisfactory in accuracy since it does not consider the meaning of the entire sentence, as it only evaluates whether the words in the list appears or not. Therefore, sometimes people use words in the list as an modal word instead of insult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning bad words on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing this, is to learn the insulting words on the fly using `CountVectorizer`. \n",
    "\n",
    "Please refer to the scikit learn tutorial at 'http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html' if you need some help.\n",
    "\n",
    "Here is what you need to do:\n",
    "\n",
    "- Import `CountVectorizer` from `sklearn.feature_extraction.text`\n",
    "- Train the `CountVectorizer` on the insults and create a feature set $X$ representing words in the comments\n",
    "- Train `MultinomialNB` and `BernoulliNB` from `scikitsklearn`  on the new feature set $X$\n",
    "- Using cross-validation, compute the accuracy, precision, recall, F1 and AUC of your model\n",
    "- What is your verdict?\n",
    "\n",
    "NOTE: The F1 score is another useful score to compute when one of the two classes is very rare. We didn't go over it in class but it's basically the harmonic mean between precision and recall and goes from 0 (min) to 1 (max).  You can see more here: 'https://en.wikipedia.org/wiki/F1_score' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial and Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Accuracy: 0.7539914328343842\n",
      "Multinomial Precision: 0.5322186916711257\n",
      "Multinomial Recall: 0.6148598769651402\n",
      "Multinomial F1 Score: 0.5705229719971298\n",
      "Multinomial AUC Score: 0.7838520003521507\n",
      "Bernoulli Accuracy: 0.727891739262967\n",
      "Bernoulli Precision: 0.46098864098864095\n",
      "Bernoulli Recall: 0.13254044201412624\n",
      "Bernoulli F1 Score: 0.2056149133884983\n",
      "Bernoulli AUC Score: 0.7471143891382318\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train = data['Comment']\n",
    "y_train = data['Insult']\n",
    "\n",
    "# Vectorize the comments\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "modelMul = MultinomialNB()\n",
    "modelBer = BernoulliNB()\n",
    "\n",
    "# Train the model\n",
    "modelMul.fit(X_vec, y_train)\n",
    "modelBer.fit(X_vec, y_train)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracyMul =  cross_val_score(modelMul, X_vec, y_train, cv=5, scoring='accuracy')\n",
    "accuracyBer =  cross_val_score(modelBer, X_vec, y_train, cv=5, scoring='accuracy')\n",
    "precisionMul = cross_val_score(modelMul, X_vec, y_train, cv=5, scoring='precision')\n",
    "precisionBer = cross_val_score(modelBer, X_vec, y_train, cv=5, scoring='precision')\n",
    "recallMul = cross_val_score(modelMul, X_vec, y_train, cv=5, scoring='recall')\n",
    "recallBer = cross_val_score(modelBer, X_vec, y_train, cv=5, scoring='recall')\n",
    "f1Mul = cross_val_score(modelMul, X_vec, y_train, cv=5, scoring='f1')\n",
    "f1Ber = cross_val_score(modelBer, X_vec, y_train, cv=5, scoring='f1')\n",
    "aucMul = cross_val_score(modelMul, X_vec, y_train, cv=5, scoring='roc_auc')\n",
    "aucBer = cross_val_score(modelBer, X_vec, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Multinomial Accuracy: {np.mean(accuracyMul)}')\n",
    "print(f'Multinomial Precision: {np.mean(precisionMul)}')\n",
    "print(f'Multinomial Recall: {np.mean(recallMul)}')\n",
    "print(f'Multinomial F1 Score: {np.mean(f1Mul)}')\n",
    "print(f'Multinomial AUC Score: {np.mean(aucMul)}')\n",
    "\n",
    "print(f'Bernoulli Accuracy: {np.mean(accuracyBer)}')\n",
    "print(f'Bernoulli Precision: {np.mean(precisionBer)}')\n",
    "print(f'Bernoulli Recall: {np.mean(recallBer)}')\n",
    "print(f'Bernoulli F1 Score: {np.mean(f1Ber)}')\n",
    "print(f'Bernoulli AUC Score: {np.mean(aucBer)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The performances in accuracy of Multinomial and Bernoulli are similar, with accuracies around 75%, which is an obvious improvement comparing to the naive classifier used above. The use of vectorizer takes every word in the training set into consideration and Multinomial and Bernoulli classifiers uses every word in a comment to evaluate whether the comment is an insult or not. This compensates the drawback I stated before, and including the context makes the prediction more accurate. However, the statistics tells that Multinomial outperforms Bernoulli since Bernoulli's precision is lower than Multinomial's, which indicates that of all instances predicted as positive by the Bernoulli classifier, a smaller proportion was actually positive. This suggests many false positives. Also, the recall of the Bernoulli classifier is quite low, which means it's missing a lot of true positive cases (high false negative rate). It only correctly identifies about 13.25% of the actual positive instances. Given that the F1 score is the harmonic mean of precision and recall, it is greatly affected by the low recall. Because recall is so low, even a moderate precision cannot bring the F1 score up significantly. Therefore, even though their accuracy are similar, Multinomial would perform better given a larger dataset with balanced data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
